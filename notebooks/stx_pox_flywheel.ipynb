{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f42d40b",
      "metadata": {},
      "source": [
        "# Stacks PoX Flywheel: Fees \u2192 Miner Rewards \u2192 BTC Bids \u2192 PoX Yields\n",
        "\n",
        "This notebook reconstructs the Stacks Proof-of-Transfer (PoX) flywheel by joining fees, miner rewards, BTC bids, and stacker yields at the burn-block (tenure) level. It relies on the Signal21 public API for market and fee data and the Hiro Stacks API for burn chain metadata.\n",
        "\n",
        "**Objectives**\n",
        "- Pull full-history STX/BTC prices, transaction fees, PoX rewards, and anchor metadata.\n",
        "- Construct a tenure-level panel with derived reward value and \\(\r",
        "ho = \f",
        "rac{\text{BTC commit}}{\text{reward value}}\\).\n",
        "- Quantify fee uplift scenarios (+10/25/50/100/200%) to estimate incremental BTC commits and PoX APY shifts.\n",
        "\n",
        "**References**\n",
        "- [Signal21 API docs](https://app.signal21.io/docs/api.html)\n",
        "- [Signal21 API access](https://signal21.github.io/docs/extras/api-access.html)\n",
        "- [Hiro Stacks API reference](https://www.hiro.so/stacks-api)\n",
        "- [Stacks fee mechanics](https://docs.stacks.co/concepts/network-fundamentals/network)\n",
        "- [Stacks mempool fee endpoint](https://www.quicknode.com/docs/stacks/v2/extended-v2-mempool-fees)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d58b2615",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running outside Colab; ensure you execute from the repo root.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    repo_path = Path('/content/stx-labs')\n",
        "    if repo_path.exists():\n",
        "        subprocess.run(['git', '-C', str(repo_path), 'reset', '--hard', 'origin/main'], check=True)\n",
        "        subprocess.run(['git', '-C', str(repo_path), 'fetch', '--all'], check=True)\n",
        "        subprocess.run(['git', '-C', str(repo_path), 'checkout', 'main'], check=True)\n",
        "        subprocess.run(['git', '-C', str(repo_path), 'pull', '--ff-only'], check=True)\n",
        "    else:\n",
        "        subprocess.run(['git', 'clone', 'https://github.com/seconds-0/stx-labs.git', str(repo_path)], check=True)\n",
        "    os.chdir(repo_path)\n",
        "    subprocess.run(['pip', 'install', '--quiet', '-r', 'requirements.txt'], check=True)\n",
        "    print('Colab environment ready: repo synced and dependencies installed.')\n",
        "else:\n",
        "    print('Running outside Colab; ensure you execute from the repo root.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187fa302",
      "metadata": {},
      "source": [
        "## 1. Configuration & Environment Checks\n",
        "\n",
        "Edit the cell below to configure date windows, retry policy, and manual overrides. Set `HIRO_API_KEY` in your environment (or in the notebook UI) before running the data acquisition cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6c823bc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from datetime import UTC, datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def _find_project_root(start: Path) -> Path:\n",
        "    for candidate in (start, *start.parents):\n",
        "        if (candidate / 'src').is_dir():\n",
        "            return candidate\n",
        "    raise RuntimeError('Unable to locate project root containing a src/ directory.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = _find_project_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "from src import config as cfg  # noqa: E402\n",
        "\n",
        "# -------- User Parameters -------- #\n",
        "WINDOW_DAYS = (30, 90, 180)\n",
        "HISTORY_DAYS = cfg.default_date_horizon_days()\n",
        "CUSTOM_START_DATE = None  # set to datetime(YYYY, M, D, tzinfo=UTC) to override history days\n",
        "END_DATE = datetime.now(UTC)\n",
        "ANALYSIS_START = CUSTOM_START_DATE or (END_DATE - timedelta(days=HISTORY_DAYS))\n",
        "FORCE_REFRESH = False  # force-refresh all caches when True\n",
        "PRICE_SYMBOLS = (\"STX-USD\", \"BTC-USD\")\n",
        "\n",
        "# Scenario assumptions\n",
        "COINBASE_STX = 1_000.0\n",
        "FEE_PER_TX_STX = 0.08\n",
        "RHO_RANGE = (0.3, 0.5, 0.7)\n",
        "UPLIFT_POINTS = (0.10, 0.25, 0.50, 1.00, 2.00)\n",
        "REWARD_BLOCKS_PER_CYCLE = 2100\n",
        "\n",
        "RAW_PATH = cfg.RAW_DATA_DIR\n",
        "CACHE_PATH = cfg.CACHE_DIR\n",
        "OUT_PATH = cfg.OUT_DIR\n",
        "for path in (RAW_PATH, CACHE_PATH, OUT_PATH):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "HIRO_API_KEY = os.getenv(cfg.HIRO_API_KEY_ENV)\n",
        "if not HIRO_API_KEY:\n",
        "    print('\u26a0\ufe0f Set HIRO_API_KEY before running Hiro API calls.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c489d1a",
      "metadata": {},
      "source": [
        "## 2. Imports & Helper Setup\n",
        "\n",
        "The helper modules live in `src/` and encapsulate Signal21/Hiro API access, caching, and scenario math."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c3f6b4f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from src import hiro, panel_builder, prices, scenarios\n",
        "from src.fees import fetch_fee_per_tx_summary, fetch_fees_by_tenure\n",
        "from src.signal21 import probe_schema\n",
        "\n",
        "pd.options.display.float_format = \"{:.6f}\".format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a184fdf",
      "metadata": {},
      "source": [
        "## 3. Parameter Summary & Cache Status\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(f\"Analysis window: {ANALYSIS_START:%Y-%m-%d %H:%M} \u2192 {END_DATE:%Y-%m-%d %H:%M} UTC\")\n",
        "print(f\"History days: {HISTORY_DAYS} (custom start: {CUSTOM_START_DATE})\")\n",
        "print(f\"Force refresh: {FORCE_REFRESH}\")\n",
        "\n",
        "from src.prices import cached_price_series\n",
        "from src.cache_utils import read_parquet\n",
        "\n",
        "for symbol in PRICE_SYMBOLS:\n",
        "    cache_df = cached_price_series(symbol)\n",
        "    if cache_df.empty:\n",
        "        print(f\"{symbol} cache: empty\")\n",
        "    else:\n",
        "        print(\n",
        "            f\"{symbol} cache: {len(cache_df):,} rows \"\n",
        "            f\"({cache_df['ts'].min():%Y-%m-%d} \u2192 {cache_df['ts'].max():%Y-%m-%d})\"\n",
        "        )\n",
        "\n",
        "fees_cache = cfg.CACHE_DIR / 'signal21' / 'fees_by_tenure_all.parquet'\n",
        "rewards_cache = cfg.CACHE_DIR / 'hiro' / 'rewards_all.parquet'\n",
        "for label, path in [('Fees cache', fees_cache), ('Rewards cache', rewards_cache)]:\n",
        "    exists = path.exists()\n",
        "    msg = \"exists\" if exists else \"missing\"\n",
        "    print(f\"{label}: {msg} ({path})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Schema Discovery (Signal21)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08e8ad45",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to inspect schema when needed\n",
        "# tx_sample = probe_schema(\"core.txs\")\n",
        "# block_sample = probe_schema(\"core.blocks\")\n",
        "# display(tx_sample.head())\n",
        "# display(block_sample.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b07c5a7",
      "metadata": {},
      "source": [
        "## 4. Data Acquisition\n",
        "\n",
        "This section ingests prices, fees, PoX rewards, and anchor metadata. Each request uses robust retry logic and caches raw payloads under `data/raw/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "105a3348",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m price_start = START_DATE \u001b[38;5;28;01mif\u001b[39;00m USE_FULL_HISTORY \u001b[38;5;28;01melse\u001b[39;00m datetime.now(UTC) - timedelta(days=\u001b[38;5;28mmax\u001b[39m(WINDOW_DAYS))\n\u001b[32m      2\u001b[39m price_end = END_DATE\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prices_df = \u001b[43mprices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_price_panel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprice_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFORCE_REFRESH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prices_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m hourly price records spanning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprices_df[\u001b[33m'\u001b[39m\u001b[33mts\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprices_df[\u001b[33m'\u001b[39m\u001b[33mts\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m prices_df.head()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/stx-labs/src/prices.py:20\u001b[39m, in \u001b[36mload_price_panel\u001b[39m\u001b[34m(start, end, frequency, force_refresh)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_price_panel\u001b[39m(\n\u001b[32m     13\u001b[39m     start: datetime,\n\u001b[32m     14\u001b[39m     end: datetime,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     force_refresh: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     18\u001b[39m ) -> pd.DataFrame:\n\u001b[32m     19\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return merged STX-USD, BTC-USD, and STX/BTC hourly data.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     stx = \u001b[43mfetch_price_series\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSTX-USD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     btc = fetch_price_series(\u001b[33m\"\u001b[39m\u001b[33mBTC-USD\u001b[39m\u001b[33m\"\u001b[39m, start, end, frequency=frequency, force_refresh=force_refresh)\n\u001b[32m     23\u001b[39m     df = (\n\u001b[32m     24\u001b[39m         stx.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mpx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mstx_usd\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     25\u001b[39m         .merge(btc.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mpx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbtc_usd\u001b[39m\u001b[33m\"\u001b[39m}), on=\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mouter\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m         .sort_values(\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/stx-labs/src/signal21.py:50\u001b[39m, in \u001b[36mfetch_price_series\u001b[39m\u001b[34m(symbol, start, end, frequency, force_refresh)\u001b[39m\n\u001b[32m     48\u001b[39m chunk_start, chunk_end = queue.pop()\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     chunk_df = \u001b[43m_fetch_price_chunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TransientHTTPError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     58\u001b[39m     span_days = (chunk_end - chunk_start).days\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/stx-labs/src/signal21.py:108\u001b[39m, in \u001b[36m_fetch_price_chunk\u001b[39m\u001b[34m(symbol, chunk_start, chunk_end, session, force_refresh)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_price_chunk\u001b[39m(\n\u001b[32m     96\u001b[39m     symbol: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     97\u001b[39m     chunk_start: datetime,\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m     force_refresh: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    102\u001b[39m ) -> pd.DataFrame:\n\u001b[32m    103\u001b[39m     params = {\n\u001b[32m    104\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m: symbol,\n\u001b[32m    105\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfrom\u001b[39m\u001b[33m\"\u001b[39m: chunk_start.strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto\u001b[39m\u001b[33m\"\u001b[39m: chunk_end.strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m    107\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     payload = \u001b[43mcached_json_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mRequestOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msignal21_price\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m            \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPRICE_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_refresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     df = pd.DataFrame(payload)\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/stx-labs/src/http_utils.py:130\u001b[39m, in \u001b[36mcached_json_request\u001b[39m\u001b[34m(opts)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;129m@retry\u001b[39m(\n\u001b[32m    120\u001b[39m     retry=retry_if_exception(_retry_condition),\n\u001b[32m    121\u001b[39m     wait=wait_exponential_jitter(\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m )\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute\u001b[39m() -> Any:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _request_once(opts)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m payload = \u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m _store_cache(cache_path, payload)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m payload\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/stx-labs/.venv/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/stx-labs/.venv/lib/python3.13/site-packages/tenacity/__init__.py:487\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    486\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/stx-labs/.venv/lib/python3.13/site-packages/tenacity/nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "cache_before = {symbol: len(prices.cached_price_series(symbol)) for symbol in PRICE_SYMBOLS}\n",
        "prices_df = prices.load_price_panel(ANALYSIS_START, END_DATE, force_refresh=FORCE_REFRESH)\n",
        "cache_after = {symbol: len(prices.cached_price_series(symbol)) for symbol in PRICE_SYMBOLS}\n",
        "\n",
        "for symbol in PRICE_SYMBOLS:\n",
        "    before = cache_before[symbol]\n",
        "    after = cache_after[symbol]\n",
        "    delta = after - before\n",
        "    print(f\"{symbol}: {after:,} cached rows (\u0394 {delta:+,})\")\n",
        "print(\n",
        "    f\"Price panel rows: {len(prices_df):,} spanning \"\n",
        "    f\"{prices_df['ts'].min()} \u2192 {prices_df['ts'].max()}\"\n",
        ")\n",
        "prices_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6700a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "fees_df = fetch_fees_by_tenure(force_refresh=FORCE_REFRESH)\n",
        "if fees_df.empty:\n",
        "    print(\"\u26a0\ufe0f No fee data returned.\")\n",
        "else:\n",
        "    print(\n",
        "        f\"Fees rows: {len(fees_df):,} burn blocks \"\n",
        "        f\"({fees_df['burn_block_height'].min()} \u2192 {fees_df['burn_block_height'].max()})\"\n",
        "    )\n",
        "fees_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd83b3a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "if fees_df.empty:\n",
        "    rewards_df = pd.DataFrame()\n",
        "else:\n",
        "    min_height = int(fees_df['burn_block_height'].min())\n",
        "    max_height = int(fees_df['burn_block_height'].max())\n",
        "    rewards_df = hiro.aggregate_rewards_by_burn_block(\n",
        "        start_height=min_height,\n",
        "        end_height=max_height,\n",
        "        force_refresh=FORCE_REFRESH,\n",
        "    )\n",
        "    print(\n",
        "        f\"Rewards rows: {len(rewards_df):,} burn blocks \"\n",
        "        f\"({rewards_df['burn_block_height'].min()} \u2192 {rewards_df['burn_block_height'].max()})\"\n",
        "    )\n",
        "rewards_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9b4e5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "if fees_df.empty:\n",
        "    anchor_df = pd.DataFrame()\n",
        "else:\n",
        "    anchor_df = hiro.collect_anchor_metadata(fees_df['burn_block_height'].astype(int), force_refresh=FORCE_REFRESH)\n",
        "    print(f\"Collected anchor metadata for {anchor_df.shape[0]} burn blocks\")\n",
        "anchor_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e34aa9",
      "metadata": {},
      "outputs": [],
      "source": [
        "cycles_df = hiro.list_pox_cycles(force_refresh=FORCE_REFRESH)\n",
        "print(f\"Retrieved {cycles_df.shape[0]} PoX cycles\")\n",
        "cycles_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d41f3b0",
      "metadata": {},
      "source": [
        "## 5. Tenure Panel Construction\n",
        "\n",
        "Join all datasets on `burn_block_height`, align prices to anchor timestamps, and derive reward value and \\(\r",
        "ho\\)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1349c29d",
      "metadata": {},
      "outputs": [],
      "source": [
        "if fees_df.empty or anchor_df.empty:\n",
        "    panel_df = pd.DataFrame()\n",
        "else:\n",
        "    panel_cfg = panel_builder.PanelConfig(coinbase_stx=COINBASE_STX)\n",
        "    panel_df = panel_builder.build_tenure_panel(\n",
        "        fees=fees_df,\n",
        "        rewards=rewards_df,\n",
        "        anchors=anchor_df,\n",
        "        prices=prices_df,\n",
        "        config=panel_cfg,\n",
        "    )\n",
        "    panel_df = panel_builder.merge_cycle_metadata(panel_df, cycles_df)\n",
        "    print(f\"Panel contains {panel_df.shape[0]} tenures\")\n",
        "panel_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095aba55",
      "metadata": {},
      "source": [
        "## 6. Validation Checks\n",
        "\n",
        "Ensure we have consistent tenure coverage, expected coinbase value, and reasonable \\(\r",
        "ho\\) ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c1c59b",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not panel_df.empty:\n",
        "    missing_fees = panel_df['fees_stx_sum'].isna().sum()\n",
        "    coinbase_anomalies = panel_df['coinbase_flag'].sum()\n",
        "    rho_div_zero = panel_df['rho_flag_div0'].sum()\n",
        "    print(\"Missing fee entries:\", missing_fees)\n",
        "    print(\"Coinbase anomalies:\", coinbase_anomalies)\n",
        "    print(\"Zero reward value entries:\", rho_div_zero)\n",
        "\n",
        "    expected_burns = panel_df['burn_block_height'].iloc[-1] - panel_df['burn_block_height'].iloc[0] + 1\n",
        "    missing_burns = expected_burns - len(panel_df['burn_block_height'].unique())\n",
        "    print(\"Missing burn heights:\", missing_burns)\n",
        "\n",
        "    sample = panel_df.sample(min(20, len(panel_df)))\n",
        "    sample[['burn_block_height', 'fees_stx_sum', 'reward_amount_sats_sum', 'rho']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99f4a183",
      "metadata": {},
      "source": [
        "## 7. Fee Analytics Per Window\n",
        "\n",
        "Compute empirical fee-per-transaction statistics across rolling windows to benchmark against the 0.08 STX/tx baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8dee625",
      "metadata": {},
      "outputs": [],
      "source": [
        "fee_stats = {}\n",
        "for window in WINDOW_DAYS:\n",
        "    stats_df = fetch_fee_per_tx_summary(window, force_refresh=FORCE_REFRESH)\n",
        "    fee_stats[window] = stats_df\n",
        "    print(f\"Window {window}d -> observations: {stats_df.shape[0]}\")\n",
        "fee_summary_df = pd.concat({f\"{w}d\": df for w, df in fee_stats.items()}, names=[\"window\", \"row\"])\n",
        "fee_summary_df.groupby(level=\"window\").agg({\"avg_fee_stx\": \"mean\", \"median_fee_stx\": \"mean\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495cc947",
      "metadata": {},
      "source": [
        "## 8. Scenario Engine\n",
        "\n",
        "Estimate the incremental transactions, BTC commits, and PoX APY shifts for fee uplifts. `stacked_supply_stx` defaults to a rolling estimate when available, otherwise falls back to 1.35B STX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd92e7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "if panel_df.empty:\n",
        "    scenario_df = pd.DataFrame()\n",
        "else:\n",
        "    recent_panel = panel_df.tail(max(3_000, len(panel_df)))\n",
        "    mean_fee_stx = recent_panel['fees_stx_sum'].mean()\n",
        "    mean_stx_btc = recent_panel['stx_btc'].mean()\n",
        "    stacked_supply_estimate = (\n",
        "        recent_panel['reward_stx_total'].rolling(REWARD_BLOCKS_PER_CYCLE).sum().dropna().iloc[-1]\n",
        "        if len(recent_panel) >= REWARD_BLOCKS_PER_CYCLE\n",
        "        else 1_350_000_000.0\n",
        "    )\n",
        "    scenario_cfg = scenarios.ScenarioConfig(\n",
        "        fee_per_tx_stx=FEE_PER_TX_STX,\n",
        "        rho_candidates=RHO_RANGE,\n",
        "        coinbase_stx=COINBASE_STX,\n",
        "        reward_cycles_blocks=REWARD_BLOCKS_PER_CYCLE,\n",
        "        stacked_supply_stx=stacked_supply_estimate,\n",
        "    )\n",
        "    scenario_df = scenarios.build_scenarios(\n",
        "        uplift_rates=UPLIFT_POINTS,\n",
        "        mean_fee_stx=mean_fee_stx,\n",
        "        mean_stx_btc=mean_stx_btc,\n",
        "        config=scenario_cfg,\n",
        "    )\n",
        "    scenario_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9355c70",
      "metadata": {},
      "source": [
        "## 9. Visualizations\n",
        "\n",
        "Produce canonical charts: time series of fees and BTC commits, \\(\r",
        "ho\\) distribution, and scatter of reward value vs. BTC commits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c52c65",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not panel_df.empty:\n",
        "    time_fig = px.line(\n",
        "        panel_df.tail(5_000),\n",
        "        x=\"burn_block_time_iso\",\n",
        "        y=[\"fees_stx_sum\", \"reward_amount_sats_sum\"],\n",
        "        labels={\"value\": \"Amount\", \"burn_block_time_iso\": \"Burn Block Time\"},\n",
        "        title=\"Tenure Fees vs. BTC Commit (Rolling)\",\n",
        "    )\n",
        "    time_fig.show()\n",
        "\n",
        "    rho_fig = px.histogram(panel_df, x=\"rho\", nbins=50, title=\"Distribution of Rho\")\n",
        "    rho_fig.show()\n",
        "\n",
        "    scatter_fig = px.scatter(\n",
        "        panel_df,\n",
        "        x=\"reward_value_sats\",\n",
        "        y=\"reward_amount_sats_sum\",\n",
        "        trendline=\"ols\",\n",
        "        title=\"BTC Commit vs. Reward Value\",\n",
        "        labels={\"reward_value_sats\": \"Reward Value (sats)\", \"reward_amount_sats_sum\": \"BTC Commit (sats)\"},\n",
        "    )\n",
        "    scatter_fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de0e6118",
      "metadata": {},
      "source": [
        "## 10. Artifact Export\n",
        "\n",
        "Persist key datasets to `./data/` and `./out/` for downstream usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dba6ecb",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not panel_df.empty:\n",
        "    panel_path = OUT_PATH / \"tenure_panel.parquet\"\n",
        "    fees_path = OUT_PATH / \"fees_by_tenure.parquet\"\n",
        "    rewards_path = OUT_PATH / \"pox_rewards.parquet\"\n",
        "    price_path = OUT_PATH / \"prices.parquet\"\n",
        "    scenario_path = OUT_PATH / \"scenario_table.csv\"\n",
        "\n",
        "    panel_df.to_parquet(panel_path, index=False)\n",
        "    fees_df.to_parquet(fees_path, index=False)\n",
        "    rewards_df.to_parquet(rewards_path, index=False)\n",
        "    prices_df.to_parquet(price_path, index=False)\n",
        "    scenario_df.to_csv(scenario_path, index=False)\n",
        "\n",
        "    print(\"Saved panel ->\", panel_path)\n",
        "    print(\"Saved fees ->\", fees_path)\n",
        "    print(\"Saved rewards ->\", rewards_path)\n",
        "    print(\"Saved prices ->\", price_path)\n",
        "    print(\"Saved scenario table ->\", scenario_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884740ae",
      "metadata": {},
      "source": [
        "## 11. Next Steps\n",
        "\n",
        "- Extend to cycle-level aggregates (sum commits, rewards, rho by PoX cycle).\n",
        "- Add pool attribution analytics by incorporating stacker addresses.\n",
        "- Compare realized fees with Hiro mempool fee estimates for additional validation.\n",
        "- Integrate notebook with Deepnote (recommended) for easy sharing; sync with this repo for reproducibility."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}