{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stacks PoX Flywheel: Fees \u2192 Miner Rewards \u2192 BTC Bids \u2192 PoX Yields\n",
        "\n",
        "This notebook reconstructs the Stacks Proof-of-Transfer (PoX) flywheel by joining fees, miner rewards, BTC bids, and stacker yields at the burn-block (tenure) level. It relies on the Signal21 public API for market and fee data and the Hiro Stacks API for burn chain metadata.\n",
        "\n",
        "**Objectives**\n",
        "- Pull full-history STX/BTC prices, transaction fees, PoX rewards, and anchor metadata.\n",
        "- Construct a tenure-level panel with derived reward value and \\(\r",
        "ho = \f",
        "rac{\text{BTC commit}}{\text{reward value}}\\).\n",
        "- Quantify fee uplift scenarios (+10/25/50/100/200%) to estimate incremental BTC commits and PoX APY shifts.\n",
        "\n",
        "**References**\n",
        "- [Signal21 API docs](https://app.signal21.io/docs/api.html)\n",
        "- [Signal21 API access](https://signal21.github.io/docs/extras/api-access.html)\n",
        "- [Hiro Stacks API reference](https://www.hiro.so/stacks-api)\n",
        "- [Stacks fee mechanics](https://docs.stacks.co/concepts/network-fundamentals/network)\n",
        "- [Stacks mempool fee endpoint](https://www.quicknode.com/docs/stacks/v2/extended-v2-mempool-fees)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    repo_path = Path('/content/stx-labs')\n",
        "    if repo_path.exists():\n",
        "        subprocess.run(['git', '-C', str(repo_path), 'fetch', '--all'], check=True)\n",
        "        subprocess.run(['git', '-C', str(repo_path), 'checkout', 'main'], check=True)\n",
        "        subprocess.run(['git', '-C', str(repo_path), 'pull', '--ff-only'], check=True)\n",
        "    else:\n",
        "        subprocess.run(['git', 'clone', 'https://github.com/seconds-0/stx-labs.git', str(repo_path)], check=True)\n",
        "    os.chdir(repo_path)\n",
        "    subprocess.run(['pip', 'install', '--quiet', '-r', 'requirements.txt'], check=True)\n",
        "    print('Colab environment ready: repo synced and dependencies installed.')\n",
        "else:\n",
        "    print('Running outside Colab; ensure you execute from the repo root.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration & Environment Checks\n",
        "\n",
        "Edit the cell below to configure date windows, retry policy, and manual overrides. Set `HIRO_API_KEY` in your environment (or in the notebook UI) before running the data acquisition cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from src import config as cfg\n",
        "\n",
        "# src.config automatically loads .env (HIRO_API_KEY, etc.)\n",
        "\n",
        "# -------- User Parameters -------- #\n",
        "WINDOW_DAYS = (30, 90, 180)\n",
        "USE_FULL_HISTORY = True  # pull as far back as available when True\n",
        "START_DATE = datetime.utcnow() - timedelta(days=5 * 365)\n",
        "END_DATE = datetime.utcnow()\n",
        "FORCE_REFRESH = False  # set True to bypass local cache\n",
        "\n",
        "# Scenario assumptions\n",
        "COINBASE_STX = 1_000.0\n",
        "FEE_PER_TX_STX = 0.08\n",
        "RHO_RANGE = (0.3, 0.5, 0.7)\n",
        "UPLIFT_POINTS = (0.10, 0.25, 0.50, 1.00, 2.00)\n",
        "REWARD_BLOCKS_PER_CYCLE = 2100\n",
        "\n",
        "RAW_PATH = cfg.RAW_DATA_DIR\n",
        "OUT_PATH = cfg.OUT_DIR\n",
        "RAW_PATH.mkdir(parents=True, exist_ok=True)\n",
        "OUT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "HIRO_API_KEY = os.getenv(cfg.HIRO_API_KEY_ENV)\n",
        "if not HIRO_API_KEY:\n",
        "    print(\"\u26a0\ufe0f Set HIRO_API_KEY before running Hiro API calls.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports & Helper Setup\n",
        "\n",
        "The helper modules live in `src/` and encapsulate Signal21/Hiro API access, caching, and scenario math."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from src import hiro, panel_builder, prices, scenarios\n",
        "from src.fees import fetch_fee_per_tx_summary, fetch_fees_by_tenure\n",
        "from src.signal21 import probe_schema\n",
        "\n",
        "pd.options.display.float_format = \"{:.6f}\".format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Schema Discovery (Signal21)\n",
        "\n",
        "Run these exploratory queries intermittently to confirm dataset layouts. Results are cached locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Uncomment to inspect schema when needed\n",
        "# tx_sample = probe_schema(\"core.txs\")\n",
        "# block_sample = probe_schema(\"core.blocks\")\n",
        "# display(tx_sample.head())\n",
        "# display(block_sample.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Acquisition\n",
        "\n",
        "This section ingests prices, fees, PoX rewards, and anchor metadata. Each request uses robust retry logic and caches raw payloads under `data/raw/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "price_start = START_DATE if USE_FULL_HISTORY else datetime.utcnow() - timedelta(days=max(WINDOW_DAYS))\n",
        "price_end = END_DATE\n",
        "prices_df = prices.load_price_panel(price_start, price_end, force_refresh=FORCE_REFRESH)\n",
        "print(f\"Loaded {len(prices_df)} hourly price records spanning {prices_df['ts'].min()} to {prices_df['ts'].max()}\")\n",
        "prices_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fees_df = fetch_fees_by_tenure(force_refresh=FORCE_REFRESH)\n",
        "print(f\"Fetched fee aggregates for {fees_df.shape[0]} burn blocks\")\n",
        "fees_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if fees_df.empty:\n",
        "    rewards_df = pd.DataFrame()\n",
        "else:\n",
        "    min_height = int(fees_df['burn_block_height'].min())\n",
        "    max_height = int(fees_df['burn_block_height'].max())\n",
        "    rewards_df = hiro.aggregate_rewards_by_burn_block(start_height=min_height, end_height=max_height, force_refresh=FORCE_REFRESH)\n",
        "    print(f\"Aggregated PoX rewards for {rewards_df.shape[0]} burn blocks\")\n",
        "rewards_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if fees_df.empty:\n",
        "    anchor_df = pd.DataFrame()\n",
        "else:\n",
        "    anchor_df = hiro.collect_anchor_metadata(fees_df['burn_block_height'].astype(int), force_refresh=FORCE_REFRESH)\n",
        "    print(f\"Collected anchor metadata for {anchor_df.shape[0]} burn blocks\")\n",
        "anchor_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cycles_df = hiro.list_pox_cycles(force_refresh=FORCE_REFRESH)\n",
        "print(f\"Retrieved {cycles_df.shape[0]} PoX cycles\")\n",
        "cycles_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tenure Panel Construction\n",
        "\n",
        "Join all datasets on `burn_block_height`, align prices to anchor timestamps, and derive reward value and \\(\r",
        "ho\\)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if fees_df.empty or anchor_df.empty:\n",
        "    panel_df = pd.DataFrame()\n",
        "else:\n",
        "    panel_cfg = panel_builder.PanelConfig(coinbase_stx=COINBASE_STX)\n",
        "    panel_df = panel_builder.build_tenure_panel(\n",
        "        fees=fees_df,\n",
        "        rewards=rewards_df,\n",
        "        anchors=anchor_df,\n",
        "        prices=prices_df,\n",
        "        config=panel_cfg,\n",
        "    )\n",
        "    panel_df = panel_builder.merge_cycle_metadata(panel_df, cycles_df)\n",
        "    print(f\"Panel contains {panel_df.shape[0]} tenures\")\n",
        "panel_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Validation Checks\n",
        "\n",
        "Ensure we have consistent tenure coverage, expected coinbase value, and reasonable \\(\r",
        "ho\\) ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not panel_df.empty:\n",
        "    missing_fees = panel_df['fees_stx_sum'].isna().sum()\n",
        "    coinbase_anomalies = panel_df['coinbase_flag'].sum()\n",
        "    rho_div_zero = panel_df['rho_flag_div0'].sum()\n",
        "    print(\"Missing fee entries:\", missing_fees)\n",
        "    print(\"Coinbase anomalies:\", coinbase_anomalies)\n",
        "    print(\"Zero reward value entries:\", rho_div_zero)\n",
        "\n",
        "    expected_burns = panel_df['burn_block_height'].iloc[-1] - panel_df['burn_block_height'].iloc[0] + 1\n",
        "    missing_burns = expected_burns - len(panel_df['burn_block_height'].unique())\n",
        "    print(\"Missing burn heights:\", missing_burns)\n",
        "\n",
        "    sample = panel_df.sample(min(20, len(panel_df)))\n",
        "    sample[['burn_block_height', 'fees_stx_sum', 'reward_amount_sats_sum', 'rho']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Fee Analytics Per Window\n",
        "\n",
        "Compute empirical fee-per-transaction statistics across rolling windows to benchmark against the 0.08 STX/tx baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fee_stats = {}\n",
        "for window in WINDOW_DAYS:\n",
        "    stats_df = fetch_fee_per_tx_summary(window, force_refresh=FORCE_REFRESH)\n",
        "    fee_stats[window] = stats_df\n",
        "    print(f\"Window {window}d -> observations: {stats_df.shape[0]}\")\n",
        "fee_summary_df = pd.concat({f\"{w}d\": df for w, df in fee_stats.items()}, names=[\"window\", \"row\"])\n",
        "fee_summary_df.groupby(level=\"window\").agg({\"avg_fee_stx\": \"mean\", \"median_fee_stx\": \"mean\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Scenario Engine\n",
        "\n",
        "Estimate the incremental transactions, BTC commits, and PoX APY shifts for fee uplifts. `stacked_supply_stx` defaults to a rolling estimate when available, otherwise falls back to 1.35B STX."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if panel_df.empty:\n",
        "    scenario_df = pd.DataFrame()\n",
        "else:\n",
        "    recent_panel = panel_df.tail(max(3_000, len(panel_df)))\n",
        "    mean_fee_stx = recent_panel['fees_stx_sum'].mean()\n",
        "    mean_stx_btc = recent_panel['stx_btc'].mean()\n",
        "    stacked_supply_estimate = (\n",
        "        recent_panel['reward_stx_total'].rolling(REWARD_BLOCKS_PER_CYCLE).sum().dropna().iloc[-1]\n",
        "        if len(recent_panel) >= REWARD_BLOCKS_PER_CYCLE\n",
        "        else 1_350_000_000.0\n",
        "    )\n",
        "    scenario_cfg = scenarios.ScenarioConfig(\n",
        "        fee_per_tx_stx=FEE_PER_TX_STX,\n",
        "        rho_candidates=RHO_RANGE,\n",
        "        coinbase_stx=COINBASE_STX,\n",
        "        reward_cycles_blocks=REWARD_BLOCKS_PER_CYCLE,\n",
        "        stacked_supply_stx=stacked_supply_estimate,\n",
        "    )\n",
        "    scenario_df = scenarios.build_scenarios(\n",
        "        uplift_rates=UPLIFT_POINTS,\n",
        "        mean_fee_stx=mean_fee_stx,\n",
        "        mean_stx_btc=mean_stx_btc,\n",
        "        config=scenario_cfg,\n",
        "    )\n",
        "    scenario_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualizations\n",
        "\n",
        "Produce canonical charts: time series of fees and BTC commits, \\(\r",
        "ho\\) distribution, and scatter of reward value vs. BTC commits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not panel_df.empty:\n",
        "    time_fig = px.line(\n",
        "        panel_df.tail(5_000),\n",
        "        x=\"burn_block_time_iso\",\n",
        "        y=[\"fees_stx_sum\", \"reward_amount_sats_sum\"],\n",
        "        labels={\"value\": \"Amount\", \"burn_block_time_iso\": \"Burn Block Time\"},\n",
        "        title=\"Tenure Fees vs. BTC Commit (Rolling)\",\n",
        "    )\n",
        "    time_fig.show()\n",
        "\n",
        "    rho_fig = px.histogram(panel_df, x=\"rho\", nbins=50, title=\"Distribution of Rho\")\n",
        "    rho_fig.show()\n",
        "\n",
        "    scatter_fig = px.scatter(\n",
        "        panel_df,\n",
        "        x=\"reward_value_sats\",\n",
        "        y=\"reward_amount_sats_sum\",\n",
        "        trendline=\"ols\",\n",
        "        title=\"BTC Commit vs. Reward Value\",\n",
        "        labels={\"reward_value_sats\": \"Reward Value (sats)\", \"reward_amount_sats_sum\": \"BTC Commit (sats)\"},\n",
        "    )\n",
        "    scatter_fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Artifact Export\n",
        "\n",
        "Persist key datasets to `./data/` and `./out/` for downstream usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not panel_df.empty:\n",
        "    panel_path = OUT_PATH / \"tenure_panel.parquet\"\n",
        "    fees_path = OUT_PATH / \"fees_by_tenure.parquet\"\n",
        "    rewards_path = OUT_PATH / \"pox_rewards.parquet\"\n",
        "    price_path = OUT_PATH / \"prices.parquet\"\n",
        "    scenario_path = OUT_PATH / \"scenario_table.csv\"\n",
        "\n",
        "    panel_df.to_parquet(panel_path, index=False)\n",
        "    fees_df.to_parquet(fees_path, index=False)\n",
        "    rewards_df.to_parquet(rewards_path, index=False)\n",
        "    prices_df.to_parquet(price_path, index=False)\n",
        "    scenario_df.to_csv(scenario_path, index=False)\n",
        "\n",
        "    print(\"Saved panel ->\", panel_path)\n",
        "    print(\"Saved fees ->\", fees_path)\n",
        "    print(\"Saved rewards ->\", rewards_path)\n",
        "    print(\"Saved prices ->\", price_path)\n",
        "    print(\"Saved scenario table ->\", scenario_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Next Steps\n",
        "\n",
        "- Extend to cycle-level aggregates (sum commits, rewards, rho by PoX cycle).\n",
        "- Add pool attribution analytics by incorporating stacker addresses.\n",
        "- Compare realized fees with Hiro mempool fee estimates for additional validation.\n",
        "- Integrate notebook with Deepnote (recommended) for easy sharing; sync with this repo for reproducibility."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}